\begin{center}
\textit{Co-Authors: Philip Salzmann}
\end{center} 
The highest tiers of performance and efficiency in contemporary large-scale HPC systems are generally achieved by accelerator clusters, which are commonly programmed with low-level or vendor-specific approaches such as MPI+CUDA. The Celerity runtime system provides a data-flow-centric high-productivity API for implementing HPC applications on such clusters, based on the established SYCL industry standard. It is designed to alleviate the development and maintenance burdens inherent in distributed memory systems as well as those introduced by accelerator programming.

A core feature of Celerity is the declarative specification of any given taskâ€™s resource requirements with so-called "range mappers", which can be provided either in terms of existing patterns, or more freely constructed with functors. Based on only this information, the Celerity system asynchronously builds a task and distributed command graph at runtime, transparently splitting kernels across multiple nodes and performing the required data transfers.


In order to implement this automatic data dependency computation and command generation, the runtime system needs to precisely track the state of each distributed data buffer and its content in the system. This imposes challenges on the algorithms and data structures employed, particularly when scaling deeper -- i.e. more time steps -- and with access patterns that generate new data and require a growing subset of it.


In this talk, we will present Task Horizons, a concept implemented in the Celerity system which allows capping the depth complexity axis of data structures with a freely configurable trade-off between structural complexity, and the level of asynchronicity possible during execution. 